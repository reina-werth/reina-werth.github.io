<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Statement | Teachable Machines</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <!-- Page Header -->
    <header class="header-wrapper">
        <h1>Project Statement</h1>
    </header>

    <!-- Navigation Links -->
    <nav class="nav-bar-horiz">
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a class="active" href="project-statement.html">Project Statement</a></li>
            <li><a href="classifying-algorithms.html">Our Algorithm</a></li>
            <li><a href="about-us.html">About Us</a></li>
        </ul>
    </nav>

    <!-- Main Content -->
    <div class="content-wrapper">
        <main>
            <h2>About the Project</h2>
            <p>When we were first introduced to this project, we found ourselves grappling with uncertainty about where to begin. How could we encapsulate the profound lessons from Dr. Joy Buolamwini’s "Unmasking AI" in a single machine learning algorithm, especially while we were still learning how to train an algorithm ourselves? This challenge forced us to reflect deeply on the tools and knowledge we had at our disposal. After some deliberation, we decided to focus on dice—a familiar object to both of us that also offered significant opportunities to create our own training set. Dice, with their varied shapes and designs, presented a tangible and accessible starting point for our algorithm. More importantly, we realized that this seemingly simple project could serve as a lens through which to examine the critical lessons from "Unmasking AI". This statement will explore those connections, organized by the key lessons learned from Dr. Buolamwini’s work.
            </p>
            <h3>Lesson 1: Testing Data and Exclusion</h3>
            <p>One of the foundational lessons from "Unmasking AI" is the importance of recognizing how exclusion in training data can perpetuate biases in algorithms. As we selected images to train our dice recognition algorithm, we were limited by the dice available to us. Most of the dice in our collection were of standard shapes and configurations—tetrahedral D4s, cubic D6s, octahedral D8s, dodecahedral D12s, and icosahedral D20s, and decahedral D10s and percentile dice. These shapes represent the "standard" forms most commonly found in tabletop gaming. However, they are by no means the only shapes that exist.
            </p>
            <img class="image" src="images/standard-dice.png" alt="Image of Dice">
            <p>For instance, some manufacturers create dice in alternative shapes for aesthetic or practical reasons. D4s, for example, are notoriously sharp and difficult to read when in their standard tetrahedral form, leading to innovations like rounded-edge or elongated designs. Other examples include thematic dice sets, such as those shaped like potion bottles or other fantasy elements, designed to immerse players in a particular narrative or aesthetic.
            </p>
            <img class="image" src="images/potion-dice.jpg" alt="Image of Potion Bottle Shaped Dice">
            <p>Because our training data contained only standard-shaped dice, our algorithm struggled to recognize or correctly classify non-standard dice. If an image of a non-standard die was passed through our algorithm, it often produced inconsistent or inaccurate predictions. This limitation highlights the broader issue Dr. Buolamwini experienced while training her Aspire Mirror project: the system failed to recognize her as a Black woman because the training data lacked sufficient representation of Black women.
            </p>
            <p>This observation underscores a fundamental truth in machine learning: a machine cannot learn what it has not been taught. While this might seem trivial when applied to dice, it becomes deeply consequential in contexts involving human beings. An algorithm that misidentifies dice is harmless; one that misidentifies people can reinforce harmful biases or perpetuate systemic discrimination. For instance, facial recognition systems have been shown to exhibit significantly higher error rates for people of color, leading to potential misidentifications in criminal justice systems or barriers to access in automated systems.
            </p>
            <p>Through our project, we gained firsthand insight into how the selection of training data can shape an algorithm's accuracy and reliability. While our dice project served as a low-stakes exploration of this principle, it illuminated the profound ethical implications of data exclusion in real-world applications.
            </p>
        <h3>Lesson 2: Who Is in the Room Matters</h3>
            <p>Dr. Buolamwini emphasizes in her work that the diversity—or lack thereof—within technology development teams directly impacts the inclusivity and effectiveness of the technologies they create. This point is echoed in other texts we studied, such as Ellen Pao’s “Tech, Heal Thyself,” which explores the systemic underrepresentation of women and marginalized groups in the tech industry. When those who are excluded from technology's development are also excluded from its design considerations, the resulting systems are unlikely to serve their needs or reflect their experiences.
            </p>
            <p>Our dice project provided a microcosm of this dynamic. If either of us had owned non-standard dice, we likely would have included them in our training data, thereby expanding the algorithm's scope and improving its accuracy for those cases. However, because we were limited to the dice we personally had access to, the algorithm’s capabilities were similarly constrained. This small-scale example mirrors a much larger issue in AI development: the demographics and experiences of developers shape the datasets they use and, by extension, the systems they build.
            </p>
            <p>In the broader context, the lack of diversity among technology creators can perpetuate systemic biases. For example, if women of color are underrepresented in AI development teams, it is far less likely that the systems they create will adequately address or even consider the unique challenges faced by this group. This lack of representation can lead to algorithms that systematically exclude or misidentify marginalized individuals, as seen in cases like biased hiring algorithms or flawed predictive policing systems.
            </p>
            <p>This lesson reinforced the importance of inclusivity—not only in the datasets we use but also in the teams that design and implement technology. As we worked on our dice recognition algorithm, we realized that even small changes in who contributes to a project can significantly influence its outcomes. While the stakes were low in our project, the implications for real-world AI systems are far-reaching.
            </p>
        <h3>Lesson 3: Not Everything Needs to Be AI-Driven</h3>
            <p>Another key takeaway from "Unmasking AI" is the idea that not every problem requires an AI-based solution. Dr. Buolamwini critiques the tendency to apply AI indiscriminately, particularly in contexts where simpler or more reliable methods would suffice. This lesson is exemplified in her discussion of a security system that used facial recognition to grant or deny tenant access to an apartment building. Such systems not only risk excluding individuals due to recognition errors but also raise serious privacy and ethical concerns.
            </p>
            <p>Our dice recognition algorithm similarly prompted us to question the necessity of AI in certain contexts. While training an algorithm to recognize dice was a valuable learning experience, we couldn’t help but wonder: Is this really a problem that needs solving? Dice recognition is a straightforward task for humans, and there are simpler ways to achieve the same result without involving machine learning. For example, a physical guidebook or a simple image database could serve as a quick reference for identifying dice shapes.
            </p>
            <p>This realization aligns with Dr. Buolamwini’s argument that AI should be used judiciously and only when it offers clear advantages over existing methods. In many cases, the drive to implement AI is motivated more by novelty or perceived innovation than by actual utility. This "AI for AI's sake" mindset can lead to unnecessary complexity, wasted resources, and systems that fail to meet user needs effectively.
            </p>
            <p>Through our project, we came to appreciate the importance of critically evaluating whether AI is the best tool for a given task. While our dice algorithm was a valuable exercise in understanding machine learning principles, it also served as a reminder that not all problems require technological solutions—especially when those solutions may introduce new challenges or risks.
            </p>
        <h3>Conclusion</h3>
            <p>Our dice recognition project provided a unique opportunity to explore the principles and pitfalls of machine learning while reflecting on the critical lessons from Dr. Buolamwini’s "Unmasking AI". By grappling with issues of data exclusion, representation, and the appropriate use of AI, we gained a deeper understanding of both the technical and ethical dimensions of algorithm development.
            </p>
            <p>Through this project, we experienced firsthand how the limitations of training data can constrain an algorithm's effectiveness, echoing the systemic biases highlighted in "Unmasking AI". We also saw how the perspectives and experiences of those involved in a project shape its outcomes, reinforcing the need for diversity in technology development. Finally, we were reminded that AI is not always the best solution and that its use should be guided by thoughtful consideration of its potential benefits and drawbacks.
            </p>
            <p>While our dice project may seem trivial compared to the profound societal challenges explored in “Unmasking AI”, it offered valuable insights into the broader implications of algorithmic design. By engaging with these lessons, we hope to contribute to a more thoughtful and inclusive approach to technology development—one that prioritizes equity, accountability, and human well-being.
            </p>
        </main>
    </div>

    <!-- Footer -->
    <footer>
        <p>&copy; 2024 Teachable Machines | Kate Walas and Reina Werth</p>
    </footer>
</body>
</html>
